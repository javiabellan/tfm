{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # TODO\n",
    ">\n",
    "> - Read [30 best practices](https://forums.fast.ai/t/30-best-practices/12344)\n",
    "> - Use correct lr scheduler\n",
    ">   - 1 Cycle Policy (superconvergence)\n",
    "> - Try better models (resnet34, resnet50)\n",
    "> - Bigger batch size\n",
    "> - Balance the data\n",
    "> - Metrics: mAP\n",
    "> - Metrics: Confusion matrix\n",
    "> - Metrics: 10-fold crossvalidation\n",
    "> - Bigger input size (480)\n",
    "> - **Use libjpeg-turbo** or **PyVips** to speed up jpeg image I/O from the disk.\n",
    "> - Use **albumentations**for GPU utilization.\n",
    "\n",
    "> # DONE\n",
    "> - Use [lr_finder](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)\n",
    "> - 2nd train unfreezed (or partially unfreezed)\n",
    "> - Fix num_workers = 4  (no es posible en windows)\n",
    "> - Set validation and test set and train\n",
    "> - Metrics\n",
    ">   - Loss\n",
    ">   - Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.1\n",
      "PyTorch Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pathlib\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"PyTorch Version: \",torchvision.__version__)\n",
    "\n",
    "# plt.xkcd();  # commic plots plt.rcdefaults() to disable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a7e9d214dce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhistologyDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"val\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"show\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mdataset_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There are\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"images in the dataset.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a7e9d214dce2>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhistologyDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"val\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"show\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mdataset_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There are\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"images in the dataset.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Data hyperparameters\n",
    "csv_file    = pathlib.Path(\"D:/Datasets/TFM/histologyDS2828/imageClasses.txt\")\n",
    "data_dir    = pathlib.Path(\"D:/Datasets/TFM/histologyDS2828/imgs\")\n",
    "val_percent = 0.3\n",
    "num_workers = 0  # 4 when is fixed\n",
    "batch_size  = 64\n",
    "num_classes = 4\n",
    "scale       = 360\n",
    "input_shape = 224  # 480\n",
    "mean        = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "std         = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "class histologyDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, csv_file, data_dir, subset):\n",
    "\t\tself.labels          = pd.read_csv(csv_file, header=None, delim_whitespace=True)\n",
    "\t\tself.labels_map      = {0 : \"conective tissue\", 1 : \"ephitelial tissue\", 2 : \"muscular tissue\", 3 : \"nervous tissue\"};\n",
    "\t\tself.data_dir        = data_dir\n",
    "\t\tself.subset          = subset\n",
    "\t\tself.data_transforms = {\n",
    "\t\t\t\"train\": transforms.Compose([\n",
    "\t\t\t\t#transforms.Resize(scale),\n",
    "\t\t\t\ttransforms.RandomResizedCrop(input_shape),\n",
    "\t\t\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\t\t\ttransforms.RandomVerticalFlip(),\n",
    "\t\t\t\ttransforms.RandomRotation(degrees=90),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize(mean, std)]),\n",
    "\t\t\t\"val\": transforms.Compose([\n",
    "\t\t\t\t#transforms.Resize(scale),\n",
    "\t\t\t\ttransforms.CenterCrop(input_shape),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize(mean, std)]),\n",
    "\t\t\t\"show\": transforms.Compose([])}\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_name = data_dir / (self.labels.iloc[idx, 0])\n",
    "\t\timage    = PIL.Image.open(img_name)\n",
    "\t\timage    = self.data_transforms[self.subset](image)\n",
    "\t\tlabel    = self.labels.iloc[idx, 1] - 1\n",
    "\t\treturn image, label\n",
    "\n",
    "dataset = {x: histologyDataset(csv_file, data_dir, x) for x in [\"train\", \"val\", \"show\"]}\n",
    "dataloader = {x: DataLoader(dataset[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "print(\"There are\", dataset_sizes[\"val\"], \"images in the dataset.\")\n",
    "\n",
    "# inputs, labels = next(iter(dataloader[\"val\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'conective', 'ephitelial', 'muscular', 'nervous'\n",
    "count = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(len(dataset[\"val\"])):\n",
    "\tclass_number = dataset[\"val\"].labels.iloc[i, 1]\n",
    "\tcount[class_number-1] += 1\n",
    "\n",
    "labels2 = [a+\": \"+str(b) for a, b in zip(labels, count)]\n",
    "plt.pie(count, labels=labels, autopct='%1.1f%%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6));\n",
    "columns = 8;\n",
    "rows = 4;\n",
    "for i in range(1, columns*rows +1):\n",
    "    idx = np.random.randint(len(dataset[\"show\"]));\n",
    "    img, lbl = dataset[\"show\"][idx]\n",
    "    \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    \n",
    "    plt.title(dataset[\"val\"].labels_map[lbl])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained    = True\n",
    "model         = torchvision.models.resnet18(pretrained=pretrained) # resnet50\n",
    "learning_rate = 0.01\n",
    "momentum      = 0.9\n",
    "weight_decay  = 1e-4\n",
    "device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def see_params_to_learn():\n",
    "    print(\"\\nParams to learn:\")\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Finetune the last layer\n",
    "def freeze():\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def partial_freeze(n):\n",
    "    for i, (name, child) in enumerate(model.named_children()):\n",
    "        if i < n:\n",
    "            print(i+1,\"frozen\\t(\",name,\")\")\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print(i+1,\"unfrozen\\t(\",name,\")\")\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "# Finetune the whole model\n",
    "def unfreeze():\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "freeze()\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes) # new layer unfreezed by default\n",
    "\n",
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model:\")\n",
    "for i, (name, child) in enumerate(model.named_children()):\n",
    "    print(\"\\t(\"+str(i+1)+\") \"+name)\n",
    "\n",
    "see_params_to_learn()\n",
    "        \n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=momentum)\n",
    "#optimizer = optim.Adam(params_to_update, lr=learning_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR finder\n",
    "\n",
    "The learning rate finder looks for the optimal learning rate to start the training. The technique is quite simple. For one epoch:\n",
    "\n",
    "- Start with a very small learning rate (around 1e-8) and increase the learning rate linearly.\n",
    "- Plot the loss at each step of LR.\n",
    "- Stop the learning rate finder when loss stops going down and starts increasing.\n",
    "\n",
    "A graph is created with the x axis having learning rates and the y axis having the losses.\n",
    "\n",
    "[Webpage](https://medium.com/coinmonks/training-neural-networks-upto-10x-faster-3246d84caacd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLR(model, optimizer, criterion, trainloader, init_value=1e-8, final_value=10):\n",
    "\n",
    "\tmodel.train() # setup model for training configuration\n",
    "\n",
    "\tnum = len(trainloader) - 1 # total number of batches\n",
    "\tmult = (final_value / init_value) ** (1/num)\n",
    "\n",
    "\tlosses = []\n",
    "\tlrs = []\n",
    "\tbest_loss = 0.\n",
    "\tavg_loss = 0.\n",
    "\tbeta = 0.98 # the value for smooth losses\n",
    "\tlr = init_value\n",
    "\n",
    "\tfor batch_num, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "\t\toptimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "\t\tbatch_num += 1 # for non zero value\n",
    "\t\tinputs, targets = inputs.to(device), targets.to(device) # convert to cuda for GPU usage\n",
    "\n",
    "\t\toptimizer.zero_grad() # clear gradients\n",
    "\t\toutputs = model(inputs) # forward pass\n",
    "\t\tloss = criterion(outputs, targets) # compute loss\n",
    "\n",
    "\t\t#Compute the smoothed loss to create a clean graph\n",
    "\t\tavg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "\t\tsmoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "\n",
    "\t\t#Record the best loss\n",
    "\t\tif smoothed_loss < best_loss or batch_num==1:\n",
    "\t\t\tbest_loss = smoothed_loss\n",
    "\n",
    "\t\t# append loss and learning rates for plotting\n",
    "\t\tlrs.append(lr) #lrs.append(math.log10(lr)) # Plot modification\n",
    "\t\tlosses.append(smoothed_loss)\n",
    "\n",
    "\t\t# Stop if the loss is exploding\n",
    "\t\tif batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# backprop for next step\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# update learning rate\n",
    "\t\tlr = mult*lr\n",
    "\n",
    "\tplt.xlabel('Learning Rates')\n",
    "\tplt.ylabel('Losses')\n",
    "\tplt.semilogx(lrs, losses) #plt.plot(lrs, losses) # Plot modification\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "findLR(model, optimizer, criterion, dataloader[\"train\"], init_value=1e-4, final_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "findLR(model, optimizer, criterion, dataloader[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live plot inspired from [this](https://github.com/stared/livelossplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = { 'train': {'loss' : [], 'acc': []},\n",
    "            'val':   {'loss' : [], 'acc': []}}\n",
    "\n",
    "def draw_plot(phase, running_loss, running_corrects):\n",
    "\n",
    "\t# Step 1: Record the metrics\n",
    "\tmetrics[phase][\"loss\"].append(running_loss)\n",
    "\tmetrics[phase][\"acc\"].append(running_corrects)\n",
    "\n",
    "\t# Step 2: Plot the metrics\n",
    "\tclear_output(wait=True)\n",
    "\t#plt.figure(figsize=figsize)\n",
    "\n",
    "\t# Loss\n",
    "\tplt.subplot(1, 2, 1)\n",
    "\tx_range = range(1, len(metrics[\"train\"][\"loss\"]) + 1)\n",
    "\tplt.plot(range(1, x_range, metrics[\"train\"][\"loss\"], label=\"training\")\n",
    "\tplt.plot(range(1, x_range, metrics[\"val\"][\"loss\"], label=\"validation\")\n",
    "\tplt.title(\"Loss\")\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(loc='center right')\n",
    "\n",
    "\t# Loss\n",
    "\tplt.subplot(1, 2, 2)\n",
    "\tx_range = range(1, len(metrics[\"train\"][\"acc\"]) + 1)\n",
    "\tplt.plot(range(1, x_range, metrics[\"train\"][\"acc\"], label=\"training\")\n",
    "\tplt.plot(range(1, x_range, metrics[\"val\"][\"acc\"], label=\"validation\")\n",
    "\tplt.title(\"Accuracy\")\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(loc='center right')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "\n",
    "\t# Decay LR by a factor of 0.1 every 7 epochs\n",
    "\tscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\tsince = time.time()\n",
    "\tbest_acc = 0.0\n",
    "\t#best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\t# Iterate epochs\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\t\tprint('-' * 10)\n",
    "\n",
    "\t\t# Each epoch has a training and validation phase\n",
    "\t\tfor phase in ['train', 'val']:\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\tscheduler.step() # Scheduling the learning rate\n",
    "\t\t\t\tmodel.train()    # Set model to training mode\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.eval()     # Set model to evaluate mode\n",
    "\n",
    "\t\t\trunning_loss     = 0.0\n",
    "\t\t\trunning_corrects = 0\n",
    "\n",
    "\t\t\t# Iterate over data\n",
    "\t\t\tfor inputs, labels in dataloader[phase]:\n",
    "\t\t\t\tinputs = inputs.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()                  # zero the parameter gradients\n",
    "\t\t\t\toutputs = model(inputs)                # forward\n",
    "\t\t\t\tpreds = torch.argmax(outputs, dim=1)   # prediction\n",
    "\t\t\t\tloss = criterion(outputs, labels)      # loss\n",
    "\t\t\t\tif phase == 'train': loss.backward()   # backward \n",
    "\t\t\t\tif phase == 'train': optimizer.step()  # optimize\n",
    "\n",
    "\t\t\t\t# statistics\n",
    "\t\t\t\trunning_loss     += loss.item() * inputs.size(0)\n",
    "\t\t\t\trunning_corrects += torch.sum(preds == labels.data)\n",
    "\t\t\t\t#live_plot(phase, running_loss, running_corrects)\n",
    "\n",
    "\t\t\tepoch_loss = running_loss / dataset_sizes[phase]\n",
    "\t\t\tepoch_acc  = running_corrects.double() / dataset_sizes[phase]\n",
    "\t\t\tprint('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "\t\t\t# deep copy the model\n",
    "\t\t\tif phase == 'val' and epoch_acc > best_acc:\n",
    "\t\t\t\tbest_acc = epoch_acc\n",
    "\t\t\t\t#best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\t\tprint()\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n",
    "\tprint('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\t# load best model weights\n",
    "\t#model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 'conective', 'ephitelial', 'muscular', 'nervous'\n",
    "\n",
    "batch   = next(iter(dataloader))\n",
    "images = batch['image'].to(device)\n",
    "labels = batch['label'].to(device)\n",
    "outputs = model(images)           # forward\n",
    "\n",
    "y_true = labels.cpu().numpy()\n",
    "y_pred = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "_, y_pred2 = torch.max(outputs, 1).cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "print(y_pred2)\n",
    "print(cm)\n",
    "\n",
    "#plt.matshow(cm)\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, cm[i, j], fontsize=18, horizontalalignment=\"center\", color=\"white\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
