{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress\n",
    "\n",
    "- **Datasets**\n",
    "  - [x] Read HIS2828 dataset\n",
    "  - [x] Read ISIC2017 dataset\n",
    "- **Tradicional features**\n",
    "  - [x] Color moment features\n",
    "  - [x] Texture features\n",
    "  - [ ] SVM with tradicional features only\n",
    "- **Deep features**: Coding network (CNN)\n",
    "  - [x] Create network architecture\n",
    "  - [ ] Train\n",
    "- **Fusion methods**\n",
    "  - [ ] CNMP (multilayer perceptron as fusion method)\n",
    "  - [ ] R feature fusion (manully fixed parameter)\n",
    "  - [ ] KPCA feature fusion\n",
    "  - [ ] SVM feature fusion (SVM as fusion method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "from skimage.feature.texture import greycomatrix, greycoprops\n",
    "from skimage.measure import shannon_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIS2828 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2828 images in the dataset.\n"
     ]
    }
   ],
   "source": [
    "class HistologyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tdata_dir_hdd = pathlib.Path(\"D:/Datasets/TFM/histologyDS2828\")\n",
    "\t\tcsv_file     = data_dir_hdd / \"imageClasses.txt\"\n",
    "\t\tcsv_df       = pd.read_csv(csv_file, header=None, delim_whitespace=True, names=['Image', 'Label'])\n",
    "\t\tmean         = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "\t\tstd          = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "        \n",
    "\t\tself.image_dir  = data_dir_hdd / \"imgs\"\n",
    "\t\tself.images     = (csv_df[\"Image\"]).values\n",
    "\t\tself.labels     = (csv_df[\"Label\"]-1).values\n",
    "\t\tself.labels_map = {0: \"conective tissue\", 1: \"ephitelial tissue\", 2: \"muscular tissue\", 3: \"nervous tissue\"}\n",
    "\t\tself.transforms = transforms.Compose([transforms.RandomCrop(420),\n",
    "                                              transforms.Resize(140),\n",
    "                                              transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.RandomVerticalFlip(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean, std)])\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_name = self.image_dir / self.images[idx]\n",
    "\t\timage = PIL.Image.open(img_name)\n",
    "\t\tif self.transforms: image = self.transforms(image)\n",
    "\t\tlabel = self.labels[idx]\n",
    "\t\treturn image, label\n",
    "    \n",
    "micro_ds    = HistologyDataset()\n",
    "print(\"There are\", len(micro_ds), \"images in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISIC2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 images in the train dataset.\n",
      "There are 150 images in the valid dataset.\n",
      "There are 600 images in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "class SkinDataset(torch.utils.data.Dataset):\n",
    "\n",
    "\tdef __init__(self, subset, transforms=False):\n",
    "\t\tdataset_dir = pathlib.Path(\"D:/Datasets/TFM/ISIC-2017\")\n",
    "\t\tcsv_file    = dataset_dir / (\"ground_truth_\"+subset+\".csv\")\n",
    "\t\tcsv_df      = pd.read_csv(csv_file)\n",
    "\n",
    "\t\tself.image_dir  = dataset_dir / (\"data_\"+subset)\n",
    "\t\tself.images     = (csv_df[\"image_id\"]+\".jpg\").values\n",
    "\t\tself.labels1    = (csv_df[\"melanoma\"]).values\n",
    "\t\tself.labels2    = (csv_df[\"seborrheic_keratosis\"]).values\n",
    "\t\tself.labels_map = {0:\"melanoma\", 1:\"seborrheic\", 2:\"healthy\"}\n",
    "\t\tself.transforms = transforms\n",
    "        \n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels1)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_name = self.image_dir / self.images[idx]\n",
    "\t\timage = PIL.Image.open(img_name)\n",
    "\t\tif self.transforms: image = self.transforms(image)\n",
    "\t\tlabel = self.labels1[idx]\n",
    "\t\treturn image, label\n",
    "\n",
    "skin_ds    = {subset: SkinDataset(subset) for subset in [\"train\", \"valid\", \"test\"]}\n",
    "{print(\"There are\", len(skin_ds[subset]), \"images in the \"+subset+\" dataset.\") for subset in [\"train\", \"valid\", \"test\"]};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture features\n",
    "\n",
    "1. First acquire the gray-level co-occurrence matrix G (2 distances, 4 angles = 2*4 = 8 matrices)\n",
    "2. Then, we employ:\n",
    "   - The angular second moment (ASM)\n",
    "   - Entropy (ENT)\n",
    "   - Contrast (CON)\n",
    "   - Correlation (COR)\n",
    "   \n",
    "\n",
    "8 matrices * 4 features each = 32 total features\n",
    "\n",
    "- [Scikit-image texture features](http://scikit-image.org/docs/0.7.0/api/skimage.feature.texture.html)\n",
    "- https://stackoverflow.com/questions/50834170/image-texture-with-skimage\n",
    "- https://stackoverflow.com/questions/51172555/greycomatrix-for-rgb-image\n",
    "- [Calculating **entropy** from GLCM of an image](https://stackoverflow.com/questions/40919936/calculating-entropy-from-glcm-of-an-image)\n",
    "- [Understanding texture properties of a grey-level co-occurrence matrix (GLCM)](https://stackoverflow.com/questions/51463436/understanding-texture-properties-of-a-grey-level-co-occurrence-matrix-glcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texture feats: [3.46578798e+09 2.42953945e+09 3.55724244e+09 2.48648134e+09\n",
      " 2.26924248e+09 2.42953945e+09 2.41551461e+09 2.48648134e+09\n",
      " 6.50161900e+06 1.36685180e+07 7.67135300e+06 1.32202980e+07\n",
      " 1.04296260e+07 1.36685180e+07 1.21967730e+07 1.32202980e+07\n",
      " 1.00000566e+00 1.00002422e+00            nan 1.00002798e+00\n",
      " 1.00215181e+00 1.00002422e+00            nan 1.00002798e+00\n",
      " 8.32118415e-01 9.91823435e-01 8.40996059e-01 9.72502707e-01\n",
      " 1.06833245e+00 9.91823435e-01 1.06200750e+00 9.72502707e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('D:/Datasets/TFM/ISIC-2017/data_train/ISIC_0000000.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "#print(\"image shape:\", img.shape)\n",
    "\n",
    "\n",
    "distances  = [1,2]\n",
    "angles     = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # 0, 45, 90, 135 degree in radians.\n",
    "glcm = greycomatrix(img, distances, angles)\n",
    "#print(\"GLCM shape:\", glcm.shape)\n",
    "\n",
    "\n",
    "properties = ['ASM', 'contrast', 'correlation']\n",
    "some_texture_feats = np.hstack([greycoprops(glcm, prop).ravel() for prop in properties])\n",
    "\n",
    "entropy_feat = [shannon_entropy(glcm[:,:,x,y])  for x in range(2)   for y in range(4)]\n",
    "\n",
    "all_texture_feats = np.hstack([some_texture_feats, entropy_feat])\n",
    "print(\"texture feats:\", all_texture_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color moment features\n",
    "- https://en.wikipedia.org/wiki/Color_moments\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.moment.html\n",
    "- https://stackoverflow.com/questions/38182087/third-order-moment-calculation-numpy\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "img      = skin_ds[\"train\"][0][0]\n",
    "np_image = np.array(img)\n",
    "\n",
    "r = np_image[:, :, 0].reshape(-1)\n",
    "g = np_image[:, :, 1].reshape(-1)\n",
    "b = np_image[:, :, 2].reshape(-1)\n",
    "\n",
    "# First color moment (Mean)\n",
    "mean_r = np.mean(r)\n",
    "mean_g = np.mean(g)\n",
    "mean_b = np.mean(b)\n",
    "\n",
    "# Second color moment (Standard deviation)\n",
    "std_r = np.std(r)\n",
    "std_g = np.std(g)\n",
    "std_b = np.std(b)\n",
    "\n",
    "# Third color moment (Skewness)\n",
    "skew_r = skew(r)\n",
    "skew_g = skew(g)\n",
    "skew_b = skew(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Network architecture\n",
    "\n",
    "Input size of `3×140×140`\n",
    "\n",
    "Layer       | Kernel | Stride | Output size\n",
    "------------|--------|--------|------------\n",
    "Convolution | 11×11  |    1   | 32×130×130\n",
    "Convolution | 11×11  |    1   | 32×120×120\n",
    "Max pooling | 5×5    |    2   | 32×58×58\n",
    "Convolution | 9×9    |    1   | 64×50×50\n",
    "Max pooling | 5×5    |    2   | 64×23×23\n",
    "Convolution | 8×8    |    1   | 128×16×16\n",
    "Convolution | 9×9    |    1   | 256×8×8\n",
    "Convolution | 8×8    |    1   | 256×1×1\n",
    "Dense       |    -   |    -   | 4×1×1\n",
    "Softmax     |    -   |    -   | 4×1×1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "class CodingNetwork(nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 140, 140)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CodingNetwork, self).__init__()\n",
    "        \n",
    "        #Input channels=3, output channels=32\n",
    "        self.conv1 = nn.Conv2d(3,  32, kernel_size=11, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=11, stride=1, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=2, padding=0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=9, stride=1, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=5, stride=2, padding=0)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=8, stride=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=9, stride=1, padding=0)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=8, stride=1, padding=0)\n",
    "\n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        \n",
    "        x = F.relu(self.conv1(x)) # Size changes from (3, 140, 140) to (32, 130, 130)\n",
    "        x = F.relu(self.conv2(x)) # Size changes from (32, 130, 130) to (32, 120, 120)\n",
    "        x = self.pool1(x)         # Size changes from (32, 120, 120) to (32, 58, 58)\n",
    "\n",
    "        x = F.relu(self.conv3(x)) # Size changes from (32, 58, 58) to (64, 50, 50)\n",
    "        x = self.pool2(x)         # Size changes from (64, 50, 50) to (64, 23, 23)\n",
    "\n",
    "        x = F.relu(self.conv4(x)) # Size changes from (64, 23, 23) to (128, 16, 16)\n",
    "        x = F.relu(self.conv5(x)) # Size changes from (128, 16, 16) to (256, 8, 8)\n",
    "        x = F.relu(self.conv6(x)) # Size changes from (256, 8, 8) to (256, 1, 1)\n",
    "\n",
    "        x = x.view(-1, 256)       # Size changes from (256, 1, 1) to (256)\n",
    "        x = self.fc(x)            # Size changes from (256) to (4)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CodingNetwork()\n",
    "\n",
    "x = torch.randn(16, 3, 140, 140)\n",
    "output = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
